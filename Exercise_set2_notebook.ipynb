{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Introduction to Data Analytics - Exercise set 2 - pandas-module</b></h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your exercises (Jupyter Notebooks or Python-files) in your course Git-project.\n",
    "Use either code comments or Jupyter Notebook markdown (text) to document which exercise you are doing and what a certain code section does! \n",
    "You can return all of these exercises in a single Jupyter Notebook, if you wish."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The datasets for these exercises have been collected from kaggle.com<br />\n",
    "(a service providing different datasets for practice)</b>\n",
    "\n",
    "<img src=\"http://srv.plab.fi/~tuomasv/data_analytics_2023_images/exercise_set_2/es2_1.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>1. import pandas and read the csv-file found in Moodle (loans.csv). Use Python coding with pandas to answer the questions.</b></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LOANS.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read the laons.csv file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLOANS.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Main\\Introduction-Data-Analytics\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Main\\Introduction-Data-Analytics\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Main\\Introduction-Data-Analytics\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Main\\Introduction-Data-Analytics\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Main\\Introduction-Data-Analytics\\.venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LOANS.csv'"
     ]
    }
   ],
   "source": [
    "# Read the laons.csv file\n",
    "df = pd.read_csv(\"LOANS.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Remove the Customer ID –column from data</li>\n",
    "<li>Print the head of the data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the customer ID column\n",
    "df = df.drop(\"Customer ID\", axis=1)\n",
    "\n",
    "# Print the head of the data, way 1\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of data, way 2 (I prefer this one)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Remove rows from the data that have <b style=\"color: red;\">a too large of a loan</b> (Current Loan Amount should be less than 99999999)</li>\n",
    "    <ul>\n",
    "        <li><b>Tip:</b> use filtering!</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from data that have larger loan than  99999999\n",
    "df = df[df[\"Current Loan Amount\"] < 99999999].sort_values(by='Current Loan Amount', ascending=True)\n",
    "\n",
    "# Use reset_index to reset index and for more understandable view\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the df with reset index\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Remove rows that have the annual income as NaN (not a number)</li>\n",
    "    <ul>\n",
    "        <li><b>Extra task:</b> use imputation to use average income as the value instead of NaN</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any na (missing values) within 'Annual Income Column\n",
    "df['Annual Income'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19154\n",
    "# Remove the rows that have annual income as Nan\n",
    "df = df.dropna(subset=[\"Annual Income\"])\n",
    "\n",
    "# Display the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average income\n",
    "average_income = df[\"Annual Income\"].mean()\n",
    "\n",
    "# Replace NaN annual income with the average income\n",
    "df['Annual Income'].fillna(df[\"Annual Income\"].mean(), inplace=True)\n",
    "df['Annual Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if imputation method worked\n",
    "df['Annual Income'].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Get the average Current Loan Amount</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average Current loan amount\n",
    "average_current_loan_amount =  df[\"Current Loan Amount\"].mean()\n",
    "print(f\"The average current loan amount is: {average_current_loan_amount}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Get the highest and lowest Annual Income in the dataset</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the  highest and lowest annual income in the dataset\n",
    "highest_annual_income = df['Annual Income'].max()\n",
    "lowest_annual_income = df['Annual Income'].min()\n",
    "print('Highest Annual Income:', highest_annual_income)\n",
    "print('Lowest Annual Income:', lowest_annual_income)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Get the Home Ownership value of the <b>Loan ID = bbf87a87-22cd-4d10-bd9b-7a9cc1b6e59d</b></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Home Ownership value of the Loan ID = bbf87a87-22cd-4d10-bd9b-7a9cc1b6e59d\n",
    "home_ownership_value = df[df['Loan ID'] == 'bbf87a87-22cd-4d10-bd9b-7a9cc1b6e59d']['Home Ownership'].iloc[0]\n",
    "print('Home Ownership:', home_ownership_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Create a new field into your dataset called <b>Actual Annual Income</b>.</li>\n",
    "<br><b>Note:</b> The Actual Annual Income follow this formula:<br>\n",
    "<b style=\"color: green\">Annual Income – 12 * Monthly Debt</b><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a field called Actual Annual Income\n",
    "df[\"Actual Annual Income\"] = df[\"Annual Income\"] - 12 * df[\"Monthly Debt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Get the Actual Annual Income of the loan with the <b>ID = 76fa89b9-e6a8-49af-afa1-8151315aba8e</b></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Actual Annual Income of the loan \n",
    "Actual_Annual_Income = df[df[\"Loan ID\"]==\"76fa89b9-e6a8-49af-afa1-8151315aba8e\"][\"Actual Annual Income\"].iloc[0]\n",
    "print(f\"Actual Annual Income:{Actual_Annual_Income}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Get the Loan ID of the loan with the smallest Actual Annual Income </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Loan ID of the loan with the smallest Actual Annual Income\n",
    "smallest_actual_income_id = df[df['Actual Annual Income'] == df['Actual Annual Income'].min()]['Loan ID'].iloc[0]\n",
    "print('Loan ID with smallest Actual Annual Income:', smallest_actual_income_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>How many loans are \"Long term\"?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Long-term Loans\n",
    "long_term_loans = df[df[\"Term\"]==\"Long Term\"].shape[0]\n",
    "print(f\"Number of Long-term loans: {long_term_loans}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>How many loaners have more than 1 bankruptcy?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaners with more than one bankrupcy\n",
    "multiple_bankrupcy = df[df[\"Bankruptcies\"]> 1].shape[0]\n",
    "print(f\"Loaners with multiple bankruptcies: {multiple_bankrupcy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>How many Short Term loans are for Home Improvements?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of short term loans for the Home improvements\n",
    "home_improvement_short_term_loan = df[(df[\"Term\"]==\"Short Term\") & (df[\"Purpose\"]==\"Home Improvements\")].shape[0]\n",
    "print(f\"Number of short term loan for the home improvements: {home_improvement_short_term_loan}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>How many unique loan purposes are there?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Unique loan purposes\n",
    "# .unique() - gives us unique values\n",
    "# .nunique() - gives the total number of unique values\n",
    "unique_loan_purposes = df[\"Purpose\"].unique()\n",
    "number_unique_loan_purposes = df[\"Purpose\"].nunique()\n",
    "print(f\"Unique loan purposes: {unique_loan_purposes}\")\n",
    "print(f\"Number of unique loan purposes: {number_unique_loan_purposes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>What are the 3 most common loan purposes?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three common loan purposes\n",
    "three_common_loan_purposes = df[\"Purpose\"].value_counts().head(3)\n",
    "print(f\"The three most common Loan purposes are: {three_common_loan_purposes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Is there a correlation between <b>Annual Income</b> and <b>Number of Open Accounts</b> or is there a correlation between <b>Number of Credit Problems</b> and <b>Bankruptcies</b>?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation problem using .corr()\n",
    "correlation1 = df[\"Annual Income\"].corr(df[\"Number of Open Accounts\"])\n",
    "correlation2 = df[\"Number of Credit Problems\"].corr(df[\"Bankruptcies\"])\n",
    "print(f\"Correlation between Annual Income and Number of Open Accounts: {correlation1}\")\n",
    "print(f\"Correlation between Number of credit problems and Bankruptices: {correlation2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This informs us that correlation between Annual Income & Number of Loans columns is quite low. This could be caused by higher income inviduals having several accounts open at this bank.\n",
    "\n",
    "Other hand, correlation between Number of Credit Problems and Bankruptcies is relatively high, this could be caused by people who have income problems being more prone for filing more bankruptcies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>2. Download the purchases.csv from Moodle, and do the following observations:</b></h4>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>What was the total price sum of the Purchase Order Number 018H2015? (14 rows in total)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read purhcases.csv\n",
    "df = pd.read_csv(\"purchases.csv\")\n",
    "\n",
    "# Display the head of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>What is the name and description of the purchased item with the Purchase Order Number 3176273?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total price sum for the PON 018H2015\n",
    "total_price_PON = df[df[\"Purchase Order Number\"]== \"018H2015\"][\"Total Price\"].sum()\n",
    "print(f\"The total price for the purchase order number is: {round(total_price_PON,2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>How many occasions (rows) of purchase data happened during the year 2013?</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows of purchased data happened during the year 2013\n",
    "purchase_2013_data = df[df[\"Purchase Date\"].str.contains(\"2013\")].shape[0]\n",
    "print(f\"The purchase data in 2013 is: {purchase_2013_data} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>What are the 5 most common Departments in the data?</li>\n",
    "<ul><li><b>Extra task: </b>What are 3 Departments using most money in the data?</li></ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five most common Departments in the data\n",
    "five_common_departments = df[\"Department Name\"].value_counts().head(5)\n",
    "print(\"Five most common Departments:\")\n",
    "print(f\"{five_common_departments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Task: Top three Department using most money\n",
    "top_three_department = df.groupby(\"Department Name\")[\"Total Price\"].sum().head(3)\n",
    "print(f\"The top three department: {top_three_department}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Sort the data by Department Name</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by Department name\n",
    "sorted_department_name = df.sort_values(\"Department Name\")\n",
    "print(sorted_department_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Small extra tasks for extra points</b></li>\n",
    "<ul>\n",
    "<li>How many purchases in the data were IT Goods and had the total price more than 50000 dollars?</li>\n",
    "<li>How many of the purchases have anything to do with IT? (IT Goods, IT Services, IT Telecommunications)</li>\n",
    "\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the $-sign and convert the value to a float\n",
    "df[\"Unit Price\"] = df[\"Unit Price\"].str.replace(\"$\", \" \").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller extra tasks\n",
    "# Number of purchases that had price more than 50000$\n",
    "purchase_over_50000 = df[(df[\"Acquisition Type\"] == \"IT Goods\") & (df[\"Total Price\"] > 50000)].shape[0]\n",
    "print(f\"The purchase in the IT goods which costs over 50000$ is: {purchase_over_50000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchases related to IT (IT Goods, IT Services, IT Telecommunications)\n",
    "it_purchases_count = df[df[\"Acquisition Type\"].str.contains(\"IT\")].shape[0]\n",
    "print(\"Number of purchases related to IT:\", it_purchases_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Other extra tasks for extra points</b></li>\n",
    "<ul>\n",
    "<li>Create a new DataFrame, where you have filtered out purchases that have a Total Price of 0 or less</li>\n",
    "<li>For this DataFrame, use groupby() –function twice to group the purchases data by Acquisition Type, and then calculating the result first by sum() and then by mean()</li>\n",
    "<li>Which two acquisition types have the largest sums and means after grouping the data?</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame, where we have filtered out purchases that have a Total Price of 0 or less\n",
    "new_dataframe = df[df['Total Price'] > 0]\n",
    "\n",
    "# Display the new dataframe\n",
    "new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupby() function\n",
    "group_purchase_data = df.groupby(\"Acquisition Type\")[\"Total Price\"].agg(['sum','mean'])\n",
    "\n",
    "# Display the grouped dataframe\n",
    "group_purchase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquisition type comparison\n",
    "largest_sum = group_purchase_data[\"sum\"].nlargest(3)\n",
    "largest_mean = group_purchase_data[\"mean\"].nlargest(3)\n",
    "print(f\"The largest sum is: {largest_sum}\")\n",
    "print(f\"The largest mean is: {largest_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output, the two acquisition types with the largest sums are \"NON-IT Services\" and \"IT Services,\" and the two acquisition types with the largest means are \"NON-IT Services\" and \"IT Services.\" Therefore, \"NON-IT Services\" is the acquisition type with the largest sum and mean, and \"IT Services\" is the acquisition type with the second-largest sum and mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://srv.plab.fi/~tuomasv/data_analytics_2023_images/exercise_set_2/es2_2.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>3. Download the data_salaries_india.csv from Moodle, and consider the following questions of the data. Use any means in pandas (or even NumPy) you wish to explain your answers.</b></h4>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Before we can do anything with the salaries, we have to convert them into something more usable</li>\n",
    "<ul>\n",
    "<li>Note: the salaries can be yearly, monthly or hourly salaries</li>\n",
    "<ul>\n",
    "<li>We don't also need the Indian rupee –sign (₹)</li>\n",
    "</ul>\n",
    "<li>You can use the template in Moodle to help you out with this (Salary filtering, pandas exercise 3)</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv-file(data_salaries_india.csv)\n",
    "df = pd.read_csv('SALARIES.csv')\n",
    "\n",
    "# Display the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all salaries(hourly, monthly) into yearly salary\n",
    "def yearly_wage(row):\n",
    "    # the last two characters determine if it's yearly, monthly, hourly\n",
    "    period = row['Salary'][-2:]\n",
    "    \n",
    "    # remove all commas and combine all numbers\n",
    "    number = int(''.join(filter(str.isdigit, row['Salary'])))\n",
    "    \n",
    "    # if it's hourly, the average work hours per year in India is\n",
    "    # approximately 2117.01. \n",
    "    if period == \"hr\":\n",
    "        number = int(number * 2117.01)\n",
    "    elif period == \"mo\":\n",
    "        # months to year\n",
    "        number = int(number * 12)\n",
    "    \n",
    "    # return the yearly salary in integer format\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Indian rupees sign\n",
    "df['Salary'] = df['Salary'].str.replace('₹', '' '')\n",
    "\n",
    "# Display the dataframe\n",
    "df['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric characters and convert to numeric\n",
    "df['Salary'] = df['Salary'].str.replace('[^\\d.]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any missing values\n",
    "missing_values = df.isna().sum()\n",
    "print(\"Number of missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing \"Company Name\" values\n",
    "df = df.dropna(subset=['Company Name'])\n",
    "\n",
    "# Display the dataframe\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>What are the most common values in different fields (Job Titles, Companies, Location)? <b>Based on the distribution, is the data balanced or not?</b></li>\n",
    "<ul>\n",
    "<li><b>Extra task:</b> there seem to be some Job Titles that are almost the same, like \"Machine Learning Data Associate\" and \"Machine Learning Associate\", combine these into something common</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common values\n",
    "# For job titles\n",
    "job_title_counts = df['Job Title'].value_counts().head(5)\n",
    "print(job_title_counts)\n",
    "\n",
    "# For company names\n",
    "company_counts = df['Company Name'].value_counts().head(5)\n",
    "print(company_counts)\n",
    "\n",
    "# For locations\n",
    "location_counts = df['Location'].value_counts().head(5)\n",
    "print(location_counts)\n",
    "\n",
    "# For salary\n",
    "salary_counts = df[\"Salary\"].value_counts().head(5)\n",
    "print(salary_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution to determine if the data is balanced\n",
    "job_title_balance = len(job_title_counts) / len(df['Job Title'].unique())\n",
    "company_balance = len(company_counts) / len(df['Company Name'].unique())\n",
    "location_balance = len(location_counts) / len(df['Location'].unique())\n",
    "\n",
    "print(job_title_balance)\n",
    "print(company_balance)\n",
    "print(location_balance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Are there any outliers in the data that might affect the averages negatively (certain salaries)? Manage the outliers as you best see fit (either remove them or leave them, based on your analysis) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of observations in each category\n",
    "job_title_counts = df['Job Title'].value_counts()\n",
    "company_counts = df['Company Name'].value_counts()\n",
    "location_counts = df['Location'].value_counts()\n",
    "salary_counts = df[\"Salary\"].value_counts()\n",
    "\n",
    "# Compare the number of observations in each category\n",
    "if job_title_counts.std() < 0.1 and company_counts.std() < 0.1 and location_counts.std() < 0.1 and salary_counts.std() < 0.1:\n",
    "    print(\"The data is balanced.\")\n",
    "else:\n",
    "    print(\"The data is imbalanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used two ways to find if the data is balanced or not. From both observation, we can conclude that the data is imbalanced.\n",
    "\n",
    "First Approach: A ratio close to 1 indicates a balanced distribution, while a ratio significantly smaller or larger than 1 suggests an imbalance in the data. In my analysis, the data for job title and company name is imbalanced whereas location data is perfectly balanced.\n",
    "\n",
    "Second Approach: The standard deviation is a measure of the spread of a dataset. In this case, I am using it to determine whether the number of observations in each category is roughly equal or not. If the standard deviation is small, it means that the counts are similar and the data is balanced. If the standard deviation is large, it means that the counts are significantly different and the data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace similar job titles with a common name\n",
    "df['Job Title'] = df['Job Title'].replace({'Machine Learning Data Associate':'Machine Learning Associate',\n",
    "                                            'Associate Machine Learning Engineer':'Machine learning Associate',\n",
    "                                            'Machine Learning Engineer':'Machine Learning Associate',\n",
    "                                            'Machine Learning Develoepr':'Machine Learning Associate',\n",
    "                                            'Machine Learning Consultant': 'Machine Learning Associate',\n",
    "                                            'Machine Learning Scientist': 'Machine Learning Associate',\n",
    "                                            'Machine Learning Associate': 'Machine Learning Data Associate I',\n",
    "                                            'Machine Learning Developer': 'Machine Learning Software Engineer',\n",
    "                                            'Senior Data Scientist':'Lead Data Scientist',\n",
    "                                            'Data Science Associate':'Data Science Consultant',\n",
    "                                            'Data Science Manager':'Data Science Consultant',\n",
    "                                            'Data Science Lead':'Lead Data Scientist',\n",
    "                                            'Data Science':'Junior Data Scientist',\n",
    "                                            'Data Scientist':'Junior Data Scientist',\n",
    "                                            'Machine Learning Associate':'Junior Data Scientist',\n",
    "                                            'Data Science Consultant':'Lead Data Scientist',\n",
    "                                            'Software Engineer - Machine Learning': 'Machine Learning Software Engineer',\n",
    "                                            'Machine Learning Data Analyst':'Data Analyst'})\n",
    "\n",
    "# Verify the changes\n",
    "job_title_counts = df['Job Title'].value_counts()\n",
    "print(job_title_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to divide the roles with different positions from Juniors to Seniors. 1. Data Analyst, Data Engineer, Data scientist - Trainee, Junior Data scientist, Lead data scientist 2. ML Associate, ML Data Associate I, ML Data Associate II, ML Software Engineer, Senior ML Engineer 3. National Director 4. CEO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>If we want to correlate upon categories (ordinal or binary), we need to use factorize(). <b>Factorize the Role-column, and add the new column to the DataFrame.</b></li>\n",
    "<ul>\n",
    "<li><b>Note:</b> using <b>factorize()</b> for nominal categories (Job Title, Location, Company Name) doesn't work well, because the numbers do not have any numeric magnitude. In other words, these categories don't measure anything, they just group data, so numerical comparison / correlation doesn't mean anything statistically.</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Z score for salary column\n",
    "df['Salary_zscore'] = (df['Salary'] - df['Salary'].mean())/df['Salary'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary greater than z_score of 3\n",
    "df[df['Salary_zscore'] < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary less than z_score of -3.\n",
    "df[df['Salary_zscore'] > -3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df for the salary based on their z score\n",
    "new_salary_df = df[(df['Salary_zscore']< 3) & (df['Salary_zscore'] > -3)]\n",
    "new_salary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score is a standard measurement that represents how many standard deviations away a data point lies from its mean. This way we can compare data points from different datasets or distributions. A z-score of 3 corresponds to a data point that is three standard deviations away from its mean. Such data points are often referred to as outliers, which should be removed. By setting a threshold of 3 for the z-scores, we exclude any data points that lie at least three standard deviations away from the mean. This ensures that the dataset contains only typical values and reduces the influence of extreme values on subsequent analyses.\n",
    "\n",
    "Originally, we had 5127 rows and after removing outliers using z score method, we have 5064 rows, which means we have successfully removed 63 outliers that might have impacted the analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Finally, check out the correlations. Does anything correlate with anything? Can we make any assumptions?</b></li>\n",
    "<ul>\n",
    "<li>Tip: When correlating against binary variables, sometimes the Spearman correlation might be more sensitive, in pandas:</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize the role column and add the new column to the DataFrame\n",
    "label1, unique1 = pd.factorize(df['Role'], sort=False)\n",
    "df['ManagerRole'] = label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize the 'Role' column\n",
    "label1, unique1 = pd.factorize(df['Role'], sort=False)\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "df.loc[:, 'ManagerRole'] = label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman correlations\n",
    "corr_managerrole_salary = df['ManagerRole'].corr(df['Salary'], method='spearman')\n",
    "corr_managerrole_sr = df['ManagerRole'].corr(df['Salaries Reported'], method='spearman')\n",
    "\n",
    "# Display calculated Spearman correlations\n",
    "print(\"Correlation between Manager Role and Salary:\", corr_managerrole_salary)\n",
    "print(\"Correlation between Manager Role and Salaries Reported:\", corr_managerrole_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation coefficient of 0.4301196078482581 between 'ManagerRole' and 'Salary' shows a considarable positive correlation. This means that as the numerical label representing the 'ManagerRole' increases, there is a tendency for the salary increase. However, the correlation is not very strong, indicating that the relationship may not be linear or there are other factors that also influence salary.\n",
    "\n",
    "On the other hand, a correlation coefficient of -0.2544565723460406 between 'ManagerRole' and 'Salaries Reported' suggests a weak negative correlation. This means that as the numerical label representing the 'ManagerRole' increases, there is a slight tendency for the reported salaries to decrease. Again, the correlation is not very strong, indicating that the relationship may not be linear or that other factors also influence reported salaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extra tasks for this dataset</b><br><br>\n",
    "\n",
    "<ul>\n",
    "<li>After all salaries have been converted to correct format by using the helper function (check Moodle), use quantiles and split the data into four different parts:</li>\n",
    "<ul>\n",
    "<li><span style=\"color: red;\">Top 25%</span><br>=> quantile(0.75)</li>\n",
    "<li><span style=\"color: red;\">Above average, values between top 25-50%</span><br>\n",
    "=> quantile(0.5)  - quantile(0.75)</li>\n",
    "<li><span style=\"color: red;\">Below average, values between top 50-75% </span><br>\n",
    "=> quantile(0.25) – quantile(0.5)\n",
    "</li>\n",
    "<li><span style=\"color: red;\">Bottom 25%</span><br>=> quantile(0.25)</li>\n",
    "<br>\n",
    "<li>What are the salary ranges for each quantile?</li><li>\n",
    "See examples in pandas-materials on how to use quantiles\n",
    "</li>\n",
    "</ul>\n",
    "</ul>\n",
    "<hr>\n",
    "\n",
    "<ul>\n",
    "<li>Did you get any ideas how this data could be improved? Do we need some particular new data or some other operations on the data? Should we filter something out based on some other column?<br><br><b>Provide arguments for your answers in code comments.</b><br><br> </li>\n",
    "<ul>\n",
    "<li>Note: There are many good possible answers here!</li>\n",
    "<li>Tip: How about replacing the \"Salaries Reported\" column with actual rows based on that number? Try doing this with the data!</li>\n",
    "<li>Remember: This data only represents data engineering salaries based on selected Indian cities.\n",
    "The world is a vast place :)\n",
    "</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the quantile of the salary column\n",
    "q1 = df['Salary'].quantile(0.25)\n",
    "q2 = df['Salary'].quantile(0.5)\n",
    "q3 = df['Salary'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into four different parts based on the quantiles\n",
    "top_25 = df[df['Salary'] > q3]\n",
    "above_avg = df[(df['Salary'] > q2) & (df['Salary'] <= q3)]\n",
    "below_avg = df[(df['Salary'] > q1) & (df['Salary'] <= q2)]\n",
    "bottom_25 = df[df['Salary'] <= q1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the salary ranges for each quantile\n",
    "print('Salary range for top 25%:', top_25['Salary'].min(), '-', top_25['Salary'].max())\n",
    "print('Salary range for above average:', above_avg['Salary'].min(), '-', above_avg['Salary'].max())\n",
    "print('Salary range for below average:', below_avg['Salary'].min(), '-', below_avg['Salary'].max())\n",
    "print('Salary range for bottom 25%:', bottom_25['Salary'].min(), '-', bottom_25['Salary'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think now looking at the data from operations perspective that every possible functions have been used for data cleaning. We have removed the indian currency(rupees) sign, removed non numeric characters, converted numeric and checked for missing values. We have also found common values within important titles, analuzed the distribution, calculated and compared the observation numbers, replaced similar job titles, calculated outliers and also factorized the columns and calculated the quantiles of the data.\n",
    "\n",
    "I could say based on the columns and information provided that one possible learnings could be that we could filter out the specific job titles or possible roles that do not have any relevance for the data analyzation. By doing this it would help simplifying the data and makes it easier to work with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2><b>Advanced extra tasks for extra points (varying challenges, some require Googling):</b></h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>1. Data merge is a useful tool when you have multiple files of data that have the exact same structure.</b><br><br>Download the two csv-files from Moodle (videogames1.csv, videogames2.csv), and combine them into one Data Frame. Lastly, save the Data Frame into a new csv-file => combined.csv.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two CSV-files in one data frame\n",
    "\n",
    "df1 = pd.read_csv(\"videogames1 (1).csv\")\n",
    "df2 = pd.read_csv(\"videogames2 (1).csv\")\n",
    "\n",
    "# Combine two files using concat function\n",
    "combined_df = pd.concat([df1,df2])\n",
    "\n",
    "# Save to new CSV file\n",
    "combined_df.to_csv(\"combined.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>2. Functions and lambdas allow us to extend the operations we wish to do to columns and rows in pandas. </b><br><br>For example, the built-in functions may not be enough in all cases. Use the data of exercise 1 (loans.csv), and create a new column called \"Income Group\" that holds a text value. <br><br>Create either a function or a lambda, that determines the True or False –value based on the Annual Income –column. Use the following table to create the values:</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"http://srv.plab.fi/~tuomasv/data_analytics_2023_images/exercise_set_2/es2_3.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>After creating the function/lambda, you can use it by using pandas' .apply() –function.<br><br></li>\n",
    "\n",
    "<li>Finally, get the amount of rows grouped by each of the new Income Group –field values and print them out.</li> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from 'LOANS.csv' into a DataFrame\n",
    "df = pd.read_csv('LOANS.csv')\n",
    "\n",
    "# Define a lambda function to determine the income group based on annual income\n",
    "determine_income_group = lambda x: (\n",
    "    '$25k or less' if x <= 25000 else \n",
    "    '$25k-$50k' if x <= 50000 else \n",
    "    '$50k-$100k' if x <= 100000 else \n",
    "    '$100k-$200k' if x <= 200000 else \n",
    "    '$200k+')\n",
    "\n",
    "# Create a new column 'Income Group' based on 'Annual Income'\n",
    "df['Income Group'] = df['Annual Income'].apply(determine_income_group)\n",
    "\n",
    "# Group by 'Income Group' and count the number of rows for each group\n",
    "income_group_counts = df.groupby('Income Group').size()\n",
    "\n",
    "# Print out the counts for each income group\n",
    "print(income_group_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>3.\tNormalization allows us to convert the values of numeric columns to be between 0 and 1.</b> This is helpful when two different numbers seem to follow the same trend, but have completely different value ranges. For example, gold and silver prices tend to follow similar patterns, but their market worth is quite different. By using normalization, we can compare these trends more easily. <br>\n",
    "\n",
    "Get historical prices of both gold and silver, and compare them without and with normalization. You can plot the prices by using df.plot() –function. Check the dataset in list in Moodle for some alternatives for gold and silver prices.\n",
    "</li>\n",
    "</ul>\n",
    "<img src=\"http://srv.plab.fi/~tuomasv/data_analytics_2023_images/exercise_set_2/es2_4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Normalization problem, I have taken data from kaggle. (https://www.kaggle.com/datasets/kapturovalexander/gold-and-silver-prices-2013-2023?resource=download). Since, we are comparing prices, the closing price or last price is used for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gold and silver price data into pandas DataFrames\n",
    "df_gold = pd.read_csv('gold_price.csv')\n",
    "df_silver = pd.read_csv('silver_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display gold dataframe\n",
    "df_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display silver dataframe\n",
    "df_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 'close' column from each DataFrame\n",
    "gold_prices = df_gold['price']\n",
    "silver_prices = df_silver['price']\n",
    "\n",
    "# Compute the minimum and maximum values of the gold and silver prices separately\n",
    "min_gold_price = gold_prices.min()\n",
    "max_gold_price = gold_prices.max()\n",
    "min_silver_price = silver_prices.min()\n",
    "max_silver_price = silver_prices.max()\n",
    "\n",
    "# Normalize the gold and silver prices independently\n",
    "gold_prices_norm = (gold_prices - min_gold_price) / (max_gold_price - min_gold_price)\n",
    "silver_prices_norm = (silver_prices - min_silver_price) / (max_silver_price - min_silver_price)\n",
    "\n",
    "# Plot the normalized gold and silver prices\n",
    "plt.plot(gold_prices_norm, label='Gold')\n",
    "plt.plot(silver_prices_norm, label='Silver')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>4.\tCreate an account to kaggle.com, and find any dataset that interests you. </b><br>\n",
    "\n",
    "There's a list of possibly interesting datasets listed in Moodle as well.\n",
    "<br><br>\n",
    "<b>Try to find interesting features in data, in particular:</b>\n",
    "</li>\n",
    "<ul>\n",
    "<li>Clean up data first (rows with too many NaN –values), values that are way too big or small, insignificant columns etc.)</li>\n",
    "<li>You can create new columns as well if it seems suitable! (either by using functions or other means)</li>\n",
    "<li>Interesting correlations (.corr() –function) and other interesting features in the data. Is something surprising in the data?</li>\n",
    "<li><b>Note: </b>There are many ways on how to approach this exercise.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<img src=\"http://srv.plab.fi/~tuomasv/data_analytics_2023_images/exercise_set_2/es2_5.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do any Kaggle.com -related extra tasks in their own Jupyter notebooks for easier coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b393597a1a01400f99fd0b0bd7e53e32f7c09a6c4e3f1d7dcfe73f5e3a50c61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
